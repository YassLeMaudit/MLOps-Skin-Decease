name: skin-mlops

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE}
  env_file:
    - .env
  environment:
    MLFLOW_TRACKING_URI: http://mlflow:5000
    MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
    AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}

    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
    AIRFLOW__CORE__ENABLE_XCOM_PICKLING: ${AIRFLOW__CORE__ENABLE_XCOM_PICKLING}
    AIRFLOW__CORE__PARALLELISM: ${AIRFLOW__CORE__PARALLELISM}
    AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
    AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
    AIRFLOW__WEBSERVER__WORKERS: ${AIRFLOW__WEBSERVER__WORKERS}
    _PIP_ADDITIONAL_REQUIREMENTS: "boto3==1.40.70 pyyaml==6.0.3 tensorflow==2.18.1 scikit-learn==1.7.2 mlflow==2.15.1"
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./src:/opt/airflow/src
    - ./data:/opt/airflow/data
  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}"
  restart: unless-stopped

services:
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    env_file:
      - .env
    ports:
      - "${MINIO_API_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9001}:9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio-mc:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_DEFAULT_BUCKETS: ${MINIO_DEFAULT_BUCKETS}
    volumes:
      - ./docker/minio/create-buckets.sh:/scripts/create-buckets.sh:ro
    entrypoint: ["/bin/sh", "/scripts/create-buckets.sh"]
    restart: "no"

  mlflow-db:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: mlflow
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow
    volumes:
      - mlflow-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mlflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  mlflow:
    build:
      context: .
      dockerfile: docker/Dockerfile.mlflow
    depends_on:
      mlflow-db:
        condition: service_healthy
      minio:
        condition: service_healthy
    env_file:
      - .env
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    command: >
      mlflow server
        --backend-store-uri ${MLFLOW_BACKEND_STORE_URI}
        --host 0.0.0.0
        --port 5000
        --default-artifact-root s3://${S3_MLFLOW_BUCKET}
    ports:
      - "${MLFLOW_PORT:-5000}:5000"
    volumes:
      - ./mlruns:/mlruns
    restart: unless-stopped

  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    env_file:
      - .env
    environment:
      MLFLOW_TRACKING_URI: http://mlflow:5000
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    depends_on:
      mlflow:
        condition: service_started
      minio:
        condition: service_healthy
    volumes:
      - .:/app
    ports:
      - "${API_PORT:-8000}:8000"
    command: uvicorn src.serving.api_fastapi:app --host 0.0.0.0 --port 8000 --reload
    restart: unless-stopped

  webapp:
    build:
      context: .
      dockerfile: docker/Dockerfile.app
    env_file:
      - .env
    environment:
      API_BASE_URL: http://api:8000
      WEBAPP_METRICS_PORT: ${WEBAPP_METRICS_PORT:-9100}
    depends_on:
      api:
        condition: service_started
    volumes:
      - .:/app
    ports:
      - "${WEBAPP_PORT:-8501}:8501"
    expose:
      - "9100"
    command: streamlit run src/webapp/app_streamlit.py --server.port=${WEBAPP_PORT:-8501} --server.address=0.0.0.0
    restart: unless-stopped

  airflow-db:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: airflow
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
    volumes:
      - airflow-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    command: redis-server --save "" --appendonly no
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate && \
        airflow users create \
          --username admin \
          --password admin \
          --firstname Local \
          --lastname Admin \
          --role Admin \
          --email admin@example.com
    depends_on:
      airflow-db:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: "no"

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "${AIRFLOW_WEB_PORT:-8080}:8080"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      airflow-db:
        condition: service_healthy

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      airflow-db:
        condition: service_healthy

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      airflow-db:
        condition: service_healthy

  prometheus:
    image: prom/prometheus:v2.53.1
    command:
      - --config.file=/etc/prometheus/prometheus.yml
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    depends_on:
      airflow-webserver:
        condition: service_started
    restart: unless-stopped

  grafana:
    image: grafana/grafana:11.2.0
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/dashboards:/etc/grafana/dashboards
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    depends_on:
      prometheus:
        condition: service_started
    restart: unless-stopped

volumes:
  airflow-db-data:
  minio-data:
  mlflow-db-data:
  prometheus-data:
  grafana-data:
